"""
æ™ºèƒ½æ–‡æ¡£åˆ†å—å™¨

å®ç°æ™ºèƒ½æ–‡æ¡£åˆ†å—ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼çš„æ··åˆåˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§ã€‚
"""

import os
import re
import sys
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import logging

# æ·»åŠ Qwen-Agentåˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent / "Qwen-Agent"))

from qwen_agent.utils.tokenization_qwen import count_tokens

from ..data_structures import ParsedDocument, DocumentChunk
from ..config import DEFAULT_CHUNK_SIZE, DEFAULT_CHUNK_OVERLAP, DOCUMENT_PROCESSING_CONFIG

logger = logging.getLogger(__name__)


# DocumentChunkç°åœ¨ä»data_structureså¯¼å…¥


class SmartChunker:
    """
    æ™ºèƒ½æ–‡æ¡£åˆ†å—å™¨

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ™ºèƒ½æ–‡æœ¬åˆ†å—ï¼ˆä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼‰
    2. å›¾ç‰‡å†…å®¹å¤„ç†ï¼ˆæ”¯æŒç®€åŒ–å’Œè¯¦ç»†æè¿°ï¼‰
    3. è¡¨æ ¼å†…å®¹å¤„ç†
    """

    def __init__(
        self,
        chunk_size: int = DEFAULT_CHUNK_SIZE,
        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,
        multimodal_llm: Optional[Any] = None,
        enable_image_description: bool = True
    ):
        """
        åˆå§‹åŒ–åˆ†å—å™¨

        Args:
            chunk_size: ç›®æ ‡å—å¤§å°ï¼ˆtokenæ•°ï¼‰
            chunk_overlap: å—é‡å å¤§å°
            multimodal_llm: å¤šæ¨¡æ€LLMå®ä¾‹ï¼Œç”¨äºå›¾åƒåˆ†æ
            enable_image_description: æ˜¯å¦å¯ç”¨è¯¦ç»†å›¾åƒæè¿°ç”Ÿæˆ
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.multimodal_llm = multimodal_llm
        self.enable_image_description = enable_image_description
    
    def chunk_document(self, parsed_doc: ParsedDocument) -> List[DocumentChunk]:
        """
        å¯¹è§£æåçš„æ–‡æ¡£è¿›è¡Œåˆ†å—
        
        Args:
            parsed_doc: è§£æåçš„æ–‡æ¡£
            
        Returns:
            æ–‡æ¡£å—åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹å¯¹æ–‡æ¡£è¿›è¡Œæ™ºèƒ½åˆ†å—: {parsed_doc.source}")
        
        chunks = []
        
        # å¤„ç†æ¯ä¸€é¡µ
        for page_idx, page in enumerate(parsed_doc.pages):
            page_chunks = self._chunk_page(page, page_idx, parsed_doc.source)
            chunks.extend(page_chunks)
        
        # å¤„ç†ç‹¬ç«‹çš„å›¾ç‰‡
        for img_idx, image in enumerate(parsed_doc.images):
            img_chunk = self._create_image_chunk(image, img_idx, parsed_doc.source)
            if img_chunk:
                chunks.append(img_chunk)
        
        logger.info(f"æ–‡æ¡£åˆ†å—å®Œæˆ: å…±ç”Ÿæˆ{len(chunks)}ä¸ªå—")
        return chunks
    
    def _chunk_page(self, page: Dict[str, Any], page_idx: int, source: str) -> List[DocumentChunk]:
        """
        å¯¹å•é¡µè¿›è¡Œåˆ†å—
        
        Args:
            page: é¡µé¢æ•°æ®
            page_idx: é¡µé¢ç´¢å¼•
            source: æ–‡æ¡£æ¥æº
            
        Returns:
            é¡µé¢å—åˆ—è¡¨
        """
        chunks = []
        current_text = ""
        current_elements = []
        
        for content_item in page.get('content', []):
            content_type = content_item.get('content_type', 'text')
            
            if content_type == 'text':
                text = content_item.get('text', '').strip()
                if text:
                    current_text += text + "\n"
                    current_elements.append(content_item)
            
            elif content_type == 'table':
                # å…ˆå¤„ç†ç´¯ç§¯çš„æ–‡æœ¬
                if current_text.strip():
                    text_chunks = self._chunk_text(
                        current_text.strip(),
                        page_idx,
                        source,
                        current_elements
                    )
                    chunks.extend(text_chunks)
                    current_text = ""
                    current_elements = []
                
                # å¤„ç†è¡¨æ ¼
                table_chunk = self._create_table_chunk(content_item, page_idx, source)
                if table_chunk:
                    chunks.append(table_chunk)
            
            elif content_type == 'image':
                # å…ˆå¤„ç†ç´¯ç§¯çš„æ–‡æœ¬
                if current_text.strip():
                    text_chunks = self._chunk_text(
                        current_text.strip(),
                        page_idx,
                        source,
                        current_elements
                    )
                    chunks.extend(text_chunks)
                    current_text = ""
                    current_elements = []
                
                # å¤„ç†å›¾ç‰‡
                img_chunk = self._create_image_chunk(content_item, page_idx, source)
                if img_chunk:
                    chunks.append(img_chunk)
        
        # å¤„ç†å‰©ä½™çš„æ–‡æœ¬
        if current_text.strip():
            text_chunks = self._chunk_text(
                current_text.strip(),
                page_idx,
                source,
                current_elements
            )
            chunks.extend(text_chunks)
        
        return chunks
    
    def _chunk_text(
        self,
        text: str,
        page_idx: int,
        source: str,
        elements: List[Dict[str, Any]]
    ) -> List[DocumentChunk]:
        """
        å¯¹æ–‡æœ¬è¿›è¡Œæ™ºèƒ½åˆ†å—

        Args:
            text: æ–‡æœ¬å†…å®¹
            page_idx: é¡µé¢ç´¢å¼•
            source: æ–‡æ¡£æ¥æº
            elements: ç›¸å…³å…ƒç´ 

        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›ä¸€ä¸ªå—
        if count_tokens(text) <= self.chunk_size:
            metadata = {
                'source': source,
                'page_num': page_idx + 1,
                'chunk_index': 0,
                'elements': elements
            }
            return [DocumentChunk(text, 'text', metadata)]

        # ç®€å•åˆ†å‰²æ–‡æœ¬ï¼ˆæŒ‰æ®µè½å’Œå¥å­ï¼‰
        chunks = []
        paragraphs = text.split('\n\n')

        current_chunk = ""
        chunk_index = 0

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°å—
            test_chunk = current_chunk + "\n\n" + paragraph if current_chunk else paragraph

            if count_tokens(test_chunk) > self.chunk_size and current_chunk:
                # ä¿å­˜å½“å‰å—
                metadata = {
                    'source': source,
                    'page_num': page_idx + 1,
                    'chunk_index': chunk_index,
                    'elements': elements
                }
                chunks.append(DocumentChunk(current_chunk.strip(), 'text', metadata))
                chunk_index += 1
                current_chunk = paragraph
            else:
                current_chunk = test_chunk

        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            metadata = {
                'source': source,
                'page_num': page_idx + 1,
                'chunk_index': chunk_index,
                'elements': elements
            }
            chunks.append(DocumentChunk(current_chunk.strip(), 'text', metadata))

        return chunks
    

    
    def _create_table_chunk(self, table_item: Dict[str, Any], page_idx: int, source: str) -> Optional[DocumentChunk]:
        """
        åˆ›å»ºè¡¨æ ¼å—
        
        Args:
            table_item: è¡¨æ ¼é¡¹
            page_idx: é¡µé¢ç´¢å¼•
            source: æ–‡æ¡£æ¥æº
            
        Returns:
            è¡¨æ ¼å—
        """
        table_content = table_item.get('table', '').strip()
        if not table_content:
            return None
        
        metadata = {
            'source': source,
            'page_num': page_idx + 1,
            'content_type': 'table',
            'bbox': table_item.get('bbox'),
            'font_size': table_item.get('font-size')
        }
        
        return DocumentChunk(table_content, 'table', metadata)
    
    def _create_image_chunk(self, image_item: Dict[str, Any], item_idx: int, source: str) -> Optional[DocumentChunk]:
        """
        åˆ›å»ºå›¾ç‰‡å—

        Args:
            image_item: å›¾ç‰‡é¡¹
            item_idx: é¡¹ç›®ç´¢å¼•
            source: æ–‡æ¡£æ¥æº

        Returns:
            å›¾ç‰‡å—
        """
        content_parts = []

        # æ·»åŠ å›¾ç‰‡åŸºæœ¬ä¿¡æ¯
        if 'image_path' in image_item:
            image_path = image_item['image_path']
            content_parts.append(f"[å›¾ç‰‡æ–‡ä»¶: {os.path.basename(image_path)}]")

            # ç”Ÿæˆå›¾ç‰‡æè¿°
            if self.enable_image_description and self.multimodal_llm:
                logger.info(f"æ­£åœ¨ç”Ÿæˆå›¾ç‰‡æè¿°: {os.path.basename(image_path)}")
                image_description = self._generate_image_description_with_llm(image_path)
                if image_description:
                    content_parts.append(f"å›¾ç‰‡å†…å®¹: {image_description}")

                    # åœ¨æ§åˆ¶å°æ˜¾ç¤ºå›¾ç‰‡æè¿°
                    print(f"\n{'='*60}")
                    print(f"ğŸ“· å›¾ç‰‡: {os.path.basename(image_path)}")
                    print(f"{'='*60}")
                    print(f"ğŸ” å›¾ç‰‡æè¿°:")
                    print(f"{image_description}")
                    print(f"{'='*60}\n")
            else:
                # ä½¿ç”¨ç®€åŒ–æè¿°
                alt_text = image_item.get('alt_text', '')
                if alt_text:
                    content_parts.append(f"å›¾ç‰‡å†…å®¹: {alt_text}")
                else:
                    content_parts.append(f"å›¾ç‰‡å†…å®¹: æ–‡æ¡£é…å›¾")

        # æ·»åŠ OCRæ–‡æœ¬
        if 'ocr_text' in image_item and image_item['ocr_text'].strip():
            content_parts.append(f"å›¾ç‰‡ä¸­çš„æ–‡å­—: {image_item['ocr_text']}")

        if not content_parts:
            return None

        content = "\n".join(content_parts)

        metadata = {
            'source': source,
            'page_num': image_item.get('page_num', item_idx + 1),
            'content_type': 'image',
            'image_path': image_item.get('image_path'),
            'alt_text': image_item.get('alt_text', ''),
            'title': image_item.get('title', '')
        }

        return DocumentChunk(content, 'image', metadata)

    def _generate_image_description_with_llm(self, image_path: str) -> str:
        """
        ç”Ÿæˆè¯¦ç»†çš„å›¾ç‰‡å†…å®¹æè¿°ï¼ˆä¼˜å…ˆä½¿ç”¨å¤šæ¨¡æ€LLMï¼‰

        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            ocr_text: OCRè¯†åˆ«çš„æ–‡æœ¬

        Returns:
            è¯¦ç»†çš„å›¾ç‰‡å†…å®¹æè¿°
        """
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨å¤šæ¨¡æ€LLMç”Ÿæˆè¯¦ç»†æè¿°
            llm_description = self._generate_image_caption_with_llm(image_path)

            if llm_description and len(llm_description.strip()) > 20:
                # å¦‚æœLLMç”Ÿæˆäº†æœ‰æ•ˆæè¿°ï¼Œä½¿ç”¨å®ƒä½œä¸ºä¸»è¦æè¿°
                enhanced_description = llm_description

                # å¦‚æœæœ‰OCRæ–‡æœ¬ï¼Œä½œä¸ºè¡¥å……ä¿¡æ¯æ·»åŠ 
                if ocr_text and len(ocr_text.strip()) > 5:
                    cleaned_ocr = ocr_text.strip()[:300]  # å¢åŠ OCRæ–‡æœ¬é•¿åº¦é™åˆ¶
                    enhanced_description += f"\n\nè¡¥å……OCRæ–‡å­—å†…å®¹ï¼š{cleaned_ocr}"

                return enhanced_description
            else:
                # å¦‚æœLLMæè¿°å¤±è´¥ï¼Œå›é€€åˆ°åŸºäºè§„åˆ™çš„æè¿°
                logger.info("å¤šæ¨¡æ€LLMæè¿°ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºäºè§„åˆ™çš„æè¿°")
                base_description = self._generate_image_description(image_path, ocr_text)

                # å¦‚æœæœ‰OCRæ–‡æœ¬ï¼Œå¢å¼ºæè¿°
                if ocr_text and len(ocr_text.strip()) > 5:
                    cleaned_ocr = ocr_text.strip()[:200]
                    enhanced_description = f"{base_description} å›¾ç‰‡ä¸­åŒ…å«çš„æ–‡å­—å†…å®¹ï¼š{cleaned_ocr}"
                    return enhanced_description

                return base_description

        except Exception as e:
            logger.warning(f"ç”Ÿæˆè¯¦ç»†å›¾ç‰‡æè¿°å¤±è´¥: {str(e)}")
            return self._generate_image_description(image_path, ocr_text)

    def _generate_image_caption_with_llm(self, image_path: str) -> str:
        """ä½¿ç”¨å¤šæ¨¡æ€LLMç”Ÿæˆå›¾åƒcaption"""
        try:
            if not self.multimodal_llm:
                logger.warning("å¤šæ¨¡æ€LLMæœªåˆå§‹åŒ–ï¼Œä½¿ç”¨åŸºäºè§„åˆ™çš„captionç”Ÿæˆ")
                return ""

            # ä½¿ç”¨qwen-vl-maxæ¨¡å‹åˆ†æå›¾åƒ
            return self._call_qwen_vl_for_caption(image_path)

        except Exception as e:
            logger.warning(f"å¤šæ¨¡æ€LLMå›¾åƒåˆ†æå¤±è´¥: {str(e)}")
            return ""

    def _call_qwen_vl_for_caption(self, image_path: str) -> str:
        """ç›´æ¥è°ƒç”¨Qwen-VLç”Ÿæˆcaption"""
        try:
            if not self.multimodal_llm:
                logger.warning("å¤šæ¨¡æ€LLMæœªåˆå§‹åŒ–")
                return ""

            if not os.path.exists(image_path):
                logger.error(f"å›¾åƒæ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return ""

            # å¯¼å…¥å¿…è¦çš„æ¨¡å—
            from qwen_agent.llm.schema import ContentItem, Message, USER, ASSISTANT

            # æ„å»ºè¯¦ç»†çš„å›¾åƒåˆ†ææç¤ºè¯
            prompt = """è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
1. ä¸»è¦å¯¹è±¡å’Œåœºæ™¯ï¼šæè¿°å›¾ç‰‡ä¸­çš„ä¸»è¦å…ƒç´ ã€äººç‰©ã€ç‰©ä½“ç­‰
2. æ–‡å­—å†…å®¹ï¼šå¦‚æœå›¾ç‰‡ä¸­æœ‰æ–‡å­—ã€æ ‡é¢˜ã€æ ‡ç­¾ç­‰ï¼Œè¯·å‡†ç¡®è¯†åˆ«å¹¶è®°å½•
3. å›¾è¡¨å’Œæ•°æ®ï¼šå¦‚æœæ˜¯å›¾è¡¨ã€è¡¨æ ¼æˆ–æŠ€æœ¯å›¾å½¢ï¼Œè¯·æè¿°å…¶ç±»å‹ã€æ•°æ®å†…å®¹ã€è¶‹åŠ¿ç­‰
4. æŠ€æœ¯ç»†èŠ‚ï¼šå¦‚æœæ˜¯æŠ€æœ¯å›¾ã€æ¶æ„å›¾ã€æµç¨‹å›¾ç­‰ï¼Œè¯·æè¿°å…¶ç»“æ„å’Œå…³é”®ä¿¡æ¯
5. è§†è§‰ç‰¹å¾ï¼šé¢œè‰²ã€å¸ƒå±€ã€é£æ ¼ç­‰é‡è¦çš„è§†è§‰ç‰¹å¾

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦å‡†ç¡®è¯¦ç»†ï¼Œæœ‰åŠ©äºåç»­çš„æ–‡æ¡£æ£€ç´¢å’Œé—®ç­”ã€‚"""

            # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
            content = [
                ContentItem(image=image_path),
                ContentItem(text=prompt)
            ]

            messages = [Message(USER, content)]

            # è°ƒç”¨å¤šæ¨¡æ€LLM
            response = None
            for response in self.multimodal_llm.chat(messages):
                continue

            if response and response[-1].role == ASSISTANT:
                result = response[-1].content.strip()
                logger.info(f"âœ… æˆåŠŸç”Ÿæˆå›¾åƒæè¿°ï¼Œé•¿åº¦: {len(result)} å­—ç¬¦")

                # æ˜¾ç¤ºæè¿°çš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
                preview = result[:100] + "..." if len(result) > 100 else result
                logger.info(f"ğŸ“ æè¿°é¢„è§ˆ: {preview}")

                return result
            else:
                logger.warning("âŒ å¤šæ¨¡æ€LLMæœªè¿”å›æœ‰æ•ˆå“åº”")
                return ""

        except Exception as e:
            logger.error(f"è°ƒç”¨Qwen-VLç”Ÿæˆcaptionå¤±è´¥: {str(e)}")
            return ""

    def _get_api_key(self) -> str:
        """è·å–APIå¯†é’¥"""
        try:
            # å°è¯•ä»å¤šä¸ªä½ç½®è·å–APIå¯†é’¥
            key_paths = [
                "dashscope_key.txt",
                "../dashscope_key.txt",
                "../../dashscope_key.txt"
            ]

            for key_path in key_paths:
                if os.path.exists(key_path):
                    with open(key_path, 'r', encoding='utf-8') as f:
                        return f.read().strip()

            # å°è¯•ä»ç¯å¢ƒå˜é‡è·å–
            import os
            return os.environ.get('DASHSCOPE_API_KEY', '')

        except Exception as e:
            logger.warning(f"è·å–APIå¯†é’¥å¤±è´¥: {str(e)}")
            return ""

    def _generate_image_description(self, image_path: str, ocr_text: str = "") -> str:
        """
        ç”Ÿæˆå›¾ç‰‡å†…å®¹æè¿°

        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            ocr_text: OCRè¯†åˆ«çš„æ–‡æœ¬

        Returns:
            å›¾ç‰‡å†…å®¹æè¿°
        """
        try:
            # åŸºäºæ–‡ä»¶åå’Œè·¯å¾„æ¨æ–­å›¾ç‰‡ç±»å‹
            filename = os.path.basename(image_path).lower()

            # æå–å›¾ç‰‡åºå·ä¿¡æ¯
            img_match = re.search(r'img_(\d+)', filename)
            img_number = int(img_match.group(1)) + 1 if img_match else None

            # åŸºäºOCRæ–‡æœ¬å†…å®¹æ¨æ–­
            if ocr_text and len(ocr_text.strip()) > 3:
                ocr_lower = ocr_text.lower()
                ocr_snippet = ocr_text.strip()[:100]  # å–å‰100ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ

                # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼ç‰¹å¾
                if any(keyword in ocr_lower for keyword in ['table', 'è¡¨', 'dataset', 'accuracy', 'precision', 'recall', 'f1', '%']):
                    return f"å›¾ç‰‡{img_number or ''}ï¼šå®éªŒæ•°æ®è¡¨æ ¼ï¼ŒåŒ…å«æ€§èƒ½æŒ‡æ ‡å’Œæ•°å€¼ç»“æœã€‚ä¸»è¦å†…å®¹ï¼š{ocr_snippet}..."

                # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾è¡¨ç‰¹å¾
                elif any(keyword in ocr_lower for keyword in ['figure', 'fig', 'å›¾', 'overview', 'architecture', 'framework']):
                    return f"å›¾ç‰‡{img_number or ''}ï¼šæŠ€æœ¯æ¶æ„å›¾æˆ–ç³»ç»Ÿæ¦‚è§ˆå›¾ã€‚ä¸»è¦å†…å®¹ï¼š{ocr_snippet}..."

                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¯¹æ¯”å†…å®¹
                elif any(keyword in ocr_lower for keyword in ['vs', 'comparison', 'baseline', 'ours', 'proposed']):
                    return f"å›¾ç‰‡{img_number or ''}ï¼šæ–¹æ³•å¯¹æ¯”å›¾è¡¨ï¼Œå±•ç¤ºä¸åŒæ¨¡å‹æˆ–æ–¹æ³•çš„æ€§èƒ½æ¯”è¾ƒã€‚ä¸»è¦å†…å®¹ï¼š{ocr_snippet}..."

                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­¦å…¬å¼
                elif any(keyword in ocr_lower for keyword in ['equation', 'formula', '=', 'âˆ‘', 'âˆ', 'loss', 'objective']):
                    return f"å›¾ç‰‡{img_number or ''}ï¼šæ•°å­¦å…¬å¼æˆ–ç®—æ³•å®šä¹‰ã€‚ä¸»è¦å†…å®¹ï¼š{ocr_snippet}..."

                # æ£€æŸ¥æ˜¯å¦åŒ…å«æµç¨‹å›¾ç‰¹å¾
                elif any(keyword in ocr_lower for keyword in ['step', 'process', 'flow', 'algorithm', 'training', 'inference']):
                    return f"å›¾ç‰‡{img_number or ''}ï¼šç®—æ³•æµç¨‹å›¾æˆ–å¤„ç†æ­¥éª¤ç¤ºæ„å›¾ã€‚ä¸»è¦å†…å®¹ï¼š{ocr_snippet}..."

                # å¦‚æœæœ‰OCRæ–‡æœ¬ä½†ä¸åŒ¹é…ç‰¹å®šç±»å‹ï¼Œç›´æ¥æè¿°å†…å®¹
                else:
                    return f"å›¾ç‰‡{img_number or ''}ï¼šåŒ…å«æ–‡å­—å†…å®¹çš„å›¾ç‰‡ã€‚ä¸»è¦å†…å®¹ï¼š{ocr_snippet}..."

            # åŸºäºæ–‡ä»¶åæ¨æ–­ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
            if any(keyword in filename for keyword in ['table', 'chart', 'graph']):
                return f"å›¾ç‰‡{img_number or ''}ï¼šè¡¨æ ¼ã€å›¾è¡¨æˆ–å›¾å½¢ï¼ŒåŒ…å«æ•°æ®ã€ç»Ÿè®¡ä¿¡æ¯æˆ–ç»“æ„åŒ–å†…å®¹ã€‚"
            elif any(keyword in filename for keyword in ['figure', 'fig', 'diagram']):
                return f"å›¾ç‰‡{img_number or ''}ï¼šå›¾å½¢æˆ–ç¤ºæ„å›¾ï¼Œå±•ç¤ºæ¦‚å¿µã€æµç¨‹æˆ–æŠ€æœ¯æ¶æ„ã€‚"
            elif any(keyword in filename for keyword in ['result', 'experiment', 'test']):
                return f"å›¾ç‰‡{img_number or ''}ï¼šå®éªŒç»“æœæˆ–æµ‹è¯•æ•°æ®çš„å¯è§†åŒ–å±•ç¤ºã€‚"
            elif any(keyword in filename for keyword in ['model', 'architecture', 'framework']):
                return f"å›¾ç‰‡{img_number or ''}ï¼šæ¨¡å‹æ¶æ„å›¾æˆ–æ¡†æ¶ç¤ºæ„å›¾ï¼Œå±•ç¤ºç³»ç»Ÿæˆ–ç®—æ³•ç»“æ„ã€‚"
            elif any(keyword in filename for keyword in ['comparison', 'compare', 'vs']):
                return f"å›¾ç‰‡{img_number or ''}ï¼šå¯¹æ¯”å›¾æˆ–æ¯”è¾ƒè¡¨ï¼Œå±•ç¤ºä¸åŒæ–¹æ³•æˆ–ç»“æœçš„å¯¹æ¯”ã€‚"
            else:
                # æ ¹æ®é¡µé¢ä½ç½®å’Œå›¾ç‰‡åºå·æ¨æ–­
                if 'page_' in filename:
                    page_match = re.search(r'page_(\d+)', filename)
                    if page_match:
                        page_num = int(page_match.group(1))
                        if page_num <= 3:
                            return f"å›¾ç‰‡{img_number or ''}ï¼šæ–‡æ¡£å¼€å¤´çš„å›¾ç‰‡ï¼Œå¯èƒ½åŒ…å«æ ‡é¢˜ã€æ‘˜è¦æˆ–ä»‹ç»æ€§å†…å®¹ã€‚"
                        elif page_num >= 20:
                            return f"å›¾ç‰‡{img_number or ''}ï¼šæ–‡æ¡£æœ«å°¾çš„å›¾ç‰‡ï¼Œå¯èƒ½åŒ…å«ç»“è®ºã€å‚è€ƒæ–‡çŒ®æˆ–é™„å½•å†…å®¹ã€‚"
                        else:
                            return f"å›¾ç‰‡{img_number or ''}ï¼šæ–‡æ¡£ç¬¬{page_num}é¡µçš„å›¾ç‰‡ï¼Œå¯èƒ½åŒ…å«ä¸»è¦å†…å®¹ã€å®éªŒç»“æœæˆ–æŠ€æœ¯ç»†èŠ‚ã€‚"

                return f"å›¾ç‰‡{img_number or ''}ï¼šåŒ…å«æ–‡æœ¬ã€å›¾è¡¨ã€ç¤ºæ„å›¾æˆ–å…¶ä»–è§†è§‰ä¿¡æ¯çš„å›¾ç‰‡ã€‚"

        except Exception as e:
            logger.warning(f"ç”Ÿæˆå›¾ç‰‡æè¿°å¤±è´¥: {str(e)}")
            return f"å›¾ç‰‡{img_number or ''}ï¼šå›¾ç‰‡å†…å®¹éœ€è¦è¿›ä¸€æ­¥åˆ†æã€‚"
