"""
ç®€åŒ–çš„æ™ºèƒ½æ–‡æ¡£åˆ†å—å™¨

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. æ™ºèƒ½æ–‡æœ¬åˆ†å—ï¼ˆä¿æŒè¯­ä¹‰å®Œæ•´æ€§ï¼‰
2. å›¾ç‰‡å†…å®¹å¤„ç†ï¼ˆæ”¯æŒLLMæè¿°ç”Ÿæˆï¼‰
3. è¡¨æ ¼å†…å®¹å¤„ç†
"""

import os
import logging
from typing import List, Dict, Any, Optional

from qwen_agent.llm.schema import ContentItem, Message, USER, ASSISTANT
from qwen_agent.utils.tokenization_qwen import count_tokens

from ..data_structures import ParsedDocument, DocumentChunk
from ..config import DEFAULT_CHUNK_SIZE, DEFAULT_CHUNK_OVERLAP

logger = logging.getLogger(__name__)


class SmartChunker:
    """ç®€åŒ–çš„æ™ºèƒ½æ–‡æ¡£åˆ†å—å™¨"""
    
    def __init__(
        self,
        chunk_size: int = DEFAULT_CHUNK_SIZE,
        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,
        multimodal_llm: Optional[Any] = None,
        enable_image_description: bool = True
    ):
        """
        åˆå§‹åŒ–åˆ†å—å™¨

        Args:
            chunk_size: ç›®æ ‡å—å¤§å°ï¼ˆtokenæ•°ï¼‰
            chunk_overlap: å—é‡å å¤§å°
            multimodal_llm: å¤šæ¨¡æ€LLMå®ä¾‹ï¼Œç”¨äºå›¾åƒåˆ†æ
            enable_image_description: æ˜¯å¦å¯ç”¨è¯¦ç»†å›¾åƒæè¿°ç”Ÿæˆ
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.multimodal_llm = multimodal_llm
        self.enable_image_description = enable_image_description

    def process_document(self, parsed_doc: ParsedDocument) -> ParsedDocument:
        """
        å¤„ç†è§£æåçš„æ–‡æ¡£ï¼Œè¿›è¡Œæ™ºèƒ½åˆ†å—

        Args:
            parsed_doc: è§£æåçš„æ–‡æ¡£

        Returns:
            åˆ†å—åçš„æ–‡æ¡£
        """
        try:
            logger.info(f"å¼€å§‹æ™ºèƒ½åˆ†å—å¤„ç†: {parsed_doc.source}")
            
            chunked_doc = ParsedDocument(parsed_doc.source, parsed_doc.doc_type)
            chunked_doc.metadata = parsed_doc.metadata.copy()
            
            # å¤„ç†æ¯ä¸ªé¡µé¢
            for page_idx, page_data in enumerate(parsed_doc.pages):
                self._process_page(page_data, page_idx, chunked_doc)
            
            logger.info(f"æ™ºèƒ½åˆ†å—å®Œæˆ: {len(chunked_doc.chunks)} ä¸ªå—")
            return chunked_doc
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½åˆ†å—å¤„ç†å¤±è´¥: {str(e)}")
            return parsed_doc

    def _process_page(self, page_data: Dict[str, Any], page_idx: int, chunked_doc: ParsedDocument):
        """å¤„ç†å•ä¸ªé¡µé¢"""
        try:
            content_items = page_data.get('content', [])
            
            for item in content_items:
                content_type = item.get('content_type', 'text')
                
                if content_type == 'text':
                    # å¤„ç†æ–‡æœ¬å†…å®¹
                    text_chunks = self._chunk_text(
                        item.get('text', ''),
                        page_idx,
                        chunked_doc.source
                    )
                    for chunk in text_chunks:
                        chunked_doc.add_chunk(chunk)
                        
                elif content_type == 'image':
                    # å¤„ç†å›¾ç‰‡å†…å®¹
                    image_chunk = self._create_image_chunk(
                        item,
                        page_idx,
                        chunked_doc.source
                    )
                    if image_chunk:
                        chunked_doc.add_chunk(image_chunk)
                        
                elif content_type == 'table':
                    # å¤„ç†è¡¨æ ¼å†…å®¹
                    table_chunk = self._create_table_chunk(
                        item,
                        page_idx,
                        chunked_doc.source
                    )
                    if table_chunk:
                        chunked_doc.add_chunk(table_chunk)
                        
        except Exception as e:
            logger.error(f"å¤„ç†é¡µé¢å¤±è´¥: {str(e)}")

    def _chunk_text(self, text: str, page_idx: int, source: str) -> List[DocumentChunk]:
        """å¯¹æ–‡æœ¬è¿›è¡Œæ™ºèƒ½åˆ†å—"""
        if not text.strip():
            return []
            
        # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›ä¸€ä¸ªå—
        if count_tokens(text) <= self.chunk_size:
            metadata = {
                'source': source,
                'page_num': page_idx + 1,
                'chunk_index': 0
            }
            return [DocumentChunk(text, 'text', metadata)]
        
        # æŒ‰æ®µè½åˆ†å‰²æ–‡æœ¬
        chunks = []
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        chunk_index = 0
        
        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue
                
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°å—
            test_chunk = current_chunk + "\n\n" + paragraph if current_chunk else paragraph
            
            if count_tokens(test_chunk) > self.chunk_size and current_chunk:
                # ä¿å­˜å½“å‰å—
                metadata = {
                    'source': source,
                    'page_num': page_idx + 1,
                    'chunk_index': chunk_index
                }
                chunks.append(DocumentChunk(current_chunk.strip(), 'text', metadata))
                chunk_index += 1
                current_chunk = paragraph
            else:
                current_chunk = test_chunk
        
        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            metadata = {
                'source': source,
                'page_num': page_idx + 1,
                'chunk_index': chunk_index
            }
            chunks.append(DocumentChunk(current_chunk.strip(), 'text', metadata))
        
        return chunks

    def _create_image_chunk(self, image_item: Dict[str, Any], page_idx: int, source: str) -> Optional[DocumentChunk]:
        """åˆ›å»ºå›¾ç‰‡å—"""
        content_parts = []

        # æ·»åŠ å›¾ç‰‡åŸºæœ¬ä¿¡æ¯
        if 'image_path' in image_item:
            image_path = image_item['image_path']
            content_parts.append(f"[å›¾ç‰‡æ–‡ä»¶: {os.path.basename(image_path)}]")

            # ç”Ÿæˆå›¾ç‰‡æè¿°
            if self.enable_image_description and self.multimodal_llm:
                logger.info(f"æ­£åœ¨ç”Ÿæˆå›¾ç‰‡æè¿°: {os.path.basename(image_path)}")
                image_description = self._generate_image_description(image_path)
                if image_description:
                    content_parts.append(f"å›¾ç‰‡å†…å®¹: {image_description}")
                    
                    # åœ¨æ§åˆ¶å°æ˜¾ç¤ºå›¾ç‰‡æè¿°
                    print(f"\n{'='*60}")
                    print(f"ğŸ“· å›¾ç‰‡: {os.path.basename(image_path)}")
                    print(f"{'='*60}")
                    print(f"ğŸ” å›¾ç‰‡æè¿°:")
                    print(f"{image_description}")
                    print(f"{'='*60}\n")
            else:
                # ä½¿ç”¨ç®€åŒ–æè¿°
                alt_text = image_item.get('alt_text', '')
                if alt_text:
                    content_parts.append(f"å›¾ç‰‡å†…å®¹: {alt_text}")
                else:
                    content_parts.append(f"å›¾ç‰‡å†…å®¹: æ–‡æ¡£é…å›¾")

        # æ·»åŠ OCRæ–‡æœ¬
        if 'ocr_text' in image_item and image_item['ocr_text'].strip():
            content_parts.append(f"å›¾ç‰‡ä¸­çš„æ–‡å­—: {image_item['ocr_text']}")

        if not content_parts:
            return None

        content = "\n".join(content_parts)

        metadata = {
            'source': source,
            'page_num': page_idx + 1,
            'content_type': 'image',
            'image_path': image_item.get('image_path'),
            'alt_text': image_item.get('alt_text', ''),
            'title': image_item.get('title', '')
        }

        return DocumentChunk(content, 'image', metadata)

    def _create_table_chunk(self, table_item: Dict[str, Any], page_idx: int, source: str) -> Optional[DocumentChunk]:
        """åˆ›å»ºè¡¨æ ¼å—"""
        content_parts = []
        
        if 'content' in table_item:
            content_parts.append(f"[è¡¨æ ¼å†…å®¹]\n{table_item['content']}")
        
        if not content_parts:
            return None
            
        content = "\n".join(content_parts)
        
        metadata = {
            'source': source,
            'page_num': page_idx + 1,
            'content_type': 'table'
        }
        
        return DocumentChunk(content, 'table', metadata)

    def _generate_image_description(self, image_path: str) -> str:
        """ä½¿ç”¨å¤šæ¨¡æ€LLMç”Ÿæˆå›¾åƒæè¿°"""
        if not self.multimodal_llm:
            logger.warning("å¤šæ¨¡æ€LLMæœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆå›¾åƒæè¿°")
            return "å›¾ç‰‡å†…å®¹"

        try:
            if not os.path.exists(image_path):
                logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
                return "å›¾ç‰‡å†…å®¹"

            # æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
            content = [
                ContentItem(image=image_path),
                ContentItem(text="è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š1. ä¸»è¦å¯¹è±¡å’Œåœºæ™¯ï¼›2. å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹ï¼›3. å›¾è¡¨ã€è¡¨æ ¼æˆ–æŠ€æœ¯å›¾å½¢çš„å…·ä½“ä¿¡æ¯ï¼›4. é¢œè‰²ã€å¸ƒå±€ç­‰è§†è§‰ç‰¹å¾ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå†…å®¹è¦å‡†ç¡®è¯¦ç»†ã€‚")
            ]

            messages = [Message(USER, content)]

            # è°ƒç”¨å¤šæ¨¡æ€LLM
            response = None
            for response in self.multimodal_llm.chat(messages):
                continue

            if response and response[-1].role == ASSISTANT:
                result = response[-1].content.strip()
                logger.info(f"âœ… æˆåŠŸç”Ÿæˆå›¾åƒæè¿°ï¼Œé•¿åº¦: {len(result)} å­—ç¬¦")
                return result
            else:
                logger.warning("âŒ å¤šæ¨¡æ€LLMæœªè¿”å›æœ‰æ•ˆå“åº”")
                return "å›¾ç‰‡å†…å®¹"

        except Exception as e:
            logger.error(f"è°ƒç”¨å¤šæ¨¡æ€LLMå¤±è´¥: {str(e)}")
            return "å›¾ç‰‡å†…å®¹"
