#!/usr/bin/env python3
"""
å­˜å‚¨æ¸…ç†å’Œç®¡ç†å·¥å…·

è¯¥è„šæœ¬ç”¨äºï¼š
1. åˆ†æå½“å‰å­˜å‚¨çŠ¶æ€
2. æ¸…ç†é‡å¤æˆ–æ— æ•ˆæ•°æ®
3. é‡æ–°ç»„ç»‡å­˜å‚¨ç»“æ„
4. éªŒè¯æ•°æ®ä¸€è‡´æ€§
"""

import os
import sys
import sqlite3
import json
import logging
from pathlib import Path
from typing import Dict, Any, List
from collections import defaultdict

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from multimodal_rag.storage.vector_store import MultimodalVectorStore
from multimodal_rag.storage.metadata_store import MetadataStore

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StorageManager:
    """å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(self, storage_path: str = "./webui_storage"):
        self.storage_path = storage_path
        self.db_path = os.path.join(storage_path, "metadata", "metadata.db")
        
        # åˆå§‹åŒ–å­˜å‚¨ç»„ä»¶
        self.metadata_store = MetadataStore(os.path.join(storage_path, "metadata"))
        self.vector_store = MultimodalVectorStore(storage_path)
    
    def analyze_storage(self) -> Dict[str, Any]:
        """åˆ†æå­˜å‚¨çŠ¶æ€"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æ€»ä½“ç»Ÿè®¡
            cursor.execute('SELECT COUNT(*) FROM metadata')
            total_count = cursor.fetchone()[0]
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            cursor.execute('SELECT chunk_type, COUNT(*) FROM metadata GROUP BY chunk_type')
            type_stats = dict(cursor.fetchall())
            
            # æŒ‰æ¥æºç»Ÿè®¡
            cursor.execute('SELECT source, COUNT(*) FROM metadata GROUP BY source')
            source_stats = dict(cursor.fetchall())
            
            # æ£€æŸ¥é‡å¤å†…å®¹
            cursor.execute('''
                SELECT content, COUNT(*) as count 
                FROM metadata 
                GROUP BY content 
                HAVING count > 1
                ORDER BY count DESC
            ''')
            duplicates = cursor.fetchall()
            
            conn.close()
            
            analysis = {
                'total_records': total_count,
                'type_distribution': type_stats,
                'source_distribution': source_stats,
                'duplicate_content_count': len(duplicates),
                'top_duplicates': duplicates[:10] if duplicates else []
            }
            
            return analysis
            
        except Exception as e:
            logger.error(f"åˆ†æå­˜å‚¨å¤±è´¥: {str(e)}")
            return {}
    
    def print_analysis(self, analysis: Dict[str, Any]):
        """æ‰“å°åˆ†æç»“æœ"""
        print("ğŸ“Š å­˜å‚¨çŠ¶æ€åˆ†æ")
        print("=" * 50)
        
        print(f"ğŸ“ˆ æ€»è®°å½•æ•°: {analysis.get('total_records', 0)}")
        
        print("\nğŸ“‹ ç±»å‹åˆ†å¸ƒ:")
        for chunk_type, count in analysis.get('type_distribution', {}).items():
            print(f"  {chunk_type}: {count}")
        
        print("\nğŸ“ æ¥æºåˆ†å¸ƒ:")
        for source, count in analysis.get('source_distribution', {}).items():
            source_name = os.path.basename(source) if source else "æœªçŸ¥"
            print(f"  {source_name}: {count}")
        
        if analysis.get('duplicate_content_count', 0) > 0:
            print(f"\nâš ï¸  å‘ç° {analysis['duplicate_content_count']} ç»„é‡å¤å†…å®¹")
            print("å‰10ä¸ªé‡å¤å†…å®¹:")
            for content, count in analysis.get('top_duplicates', []):
                print(f"  é‡å¤{count}æ¬¡: {content[:50]}...")
    
    def clean_duplicates(self) -> int:
        """æ¸…ç†é‡å¤æ•°æ®"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # æŸ¥æ‰¾é‡å¤è®°å½•
            cursor.execute('''
                SELECT content, MIN(rowid) as keep_id, COUNT(*) as count
                FROM metadata 
                GROUP BY content 
                HAVING count > 1
            ''')
            
            duplicates = cursor.fetchall()
            total_removed = 0
            
            for content, keep_id, count in duplicates:
                # åˆ é™¤é™¤äº†æœ€æ—©è®°å½•ä¹‹å¤–çš„æ‰€æœ‰é‡å¤è®°å½•
                cursor.execute('''
                    DELETE FROM metadata 
                    WHERE content = ? AND rowid != ?
                ''', (content, keep_id))
                
                removed = cursor.rowcount
                total_removed += removed
                logger.info(f"åˆ é™¤äº† {removed} ä¸ªé‡å¤è®°å½•: {content[:50]}...")
            
            conn.commit()
            conn.close()
            
            logger.info(f"æ€»å…±åˆ é™¤äº† {total_removed} ä¸ªé‡å¤è®°å½•")
            return total_removed
            
        except Exception as e:
            logger.error(f"æ¸…ç†é‡å¤æ•°æ®å¤±è´¥: {str(e)}")
            return 0
    
    def clean_by_source(self, source_pattern: str = None):
        """æŒ‰æ¥æºæ¸…ç†æ•°æ®"""
        try:
            analysis = self.analyze_storage()
            sources = list(analysis.get('source_distribution', {}).keys())
            
            if not sources:
                print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®æº")
                return
            
            print("\nğŸ“ å½“å‰æ•°æ®æº:")
            for i, source in enumerate(sources):
                source_name = os.path.basename(source) if source else "æœªçŸ¥"
                count = analysis['source_distribution'][source]
                print(f"  {i+1}. {source_name} ({count} æ¡è®°å½•)")
            
            if source_pattern:
                # è‡ªåŠ¨æ¸…ç†åŒ¹é…çš„æ¥æº
                to_remove = [s for s in sources if source_pattern in s]
            else:
                # äº¤äº’å¼é€‰æ‹©
                print("\nè¯·é€‰æ‹©è¦åˆ é™¤çš„æ•°æ®æº (è¾“å…¥ç¼–å·ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œæˆ–è¾“å…¥ 'all' æ¸…ç©ºæ‰€æœ‰):")
                choice = input().strip()
                
                if choice.lower() == 'all':
                    to_remove = sources
                else:
                    try:
                        indices = [int(x.strip()) - 1 for x in choice.split(',')]
                        to_remove = [sources[i] for i in indices if 0 <= i < len(sources)]
                    except:
                        print("æ— æ•ˆè¾“å…¥")
                        return
            
            # åˆ é™¤é€‰ä¸­çš„æ¥æº
            for source in to_remove:
                source_name = os.path.basename(source) if source else "æœªçŸ¥"
                print(f"ğŸ—‘ï¸  åˆ é™¤æ•°æ®æº: {source_name}")
                self.vector_store.delete_by_source(source)
            
            if to_remove:
                print("âœ… æ•°æ®æºåˆ é™¤å®Œæˆï¼Œå»ºè®®é‡å»ºå‘é‡ç´¢å¼•")
                
        except Exception as e:
            logger.error(f"æŒ‰æ¥æºæ¸…ç†å¤±è´¥: {str(e)}")
    
    def rebuild_indexes(self):
        """é‡å»ºå‘é‡ç´¢å¼•"""
        try:
            print("ğŸ”„ é‡å»ºå‘é‡ç´¢å¼•...")
            
            # è·å–æ‰€æœ‰æ•°æ®
            all_metadata = self.metadata_store.get_all_metadata()
            if not all_metadata:
                print("æ²¡æœ‰æ•°æ®éœ€è¦é‡å»ºç´¢å¼•")
                return
            
            # æ¸…ç©ºç°æœ‰ç´¢å¼•
            self.vector_store.clear()
            
            # æŒ‰æ¥æºåˆ†ç»„é‡å»º
            sources = set(m.get('source') for m in all_metadata)
            
            for source in sources:
                if not source:
                    continue
                    
                source_data = [m for m in all_metadata if m.get('source') == source]
                print(f"é‡å»ºç´¢å¼•: {os.path.basename(source)} ({len(source_data)} æ¡è®°å½•)")
                
                # è¿™é‡Œéœ€è¦é‡æ–°åˆ›å»ºDocumentChunkå¯¹è±¡
                from multimodal_rag.processors.smart_chunker import DocumentChunk
                
                chunks = []
                for metadata in source_data:
                    chunk = DocumentChunk(
                        content=metadata.get('content', ''),
                        chunk_type=metadata.get('chunk_type', 'text'),
                        metadata=metadata,
                        token_count=metadata.get('token_count', 0)
                    )
                    chunks.append(chunk)
                
                self.vector_store.add_chunks(chunks, source)
            
            print("âœ… å‘é‡ç´¢å¼•é‡å»ºå®Œæˆ")
            
        except Exception as e:
            logger.error(f"é‡å»ºç´¢å¼•å¤±è´¥: {str(e)}")
    
    def validate_storage(self) -> bool:
        """éªŒè¯å­˜å‚¨ä¸€è‡´æ€§"""
        try:
            print("ğŸ” éªŒè¯å­˜å‚¨ä¸€è‡´æ€§...")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = [
                os.path.join(self.storage_path, "text_index.faiss"),
                os.path.join(self.storage_path, "image_index.faiss"),
                self.db_path
            ]
            
            missing_files = [f for f in required_files if not os.path.exists(f)]
            if missing_files:
                print(f"âŒ ç¼ºå°‘æ–‡ä»¶: {missing_files}")
                return False
            
            # æ£€æŸ¥æ•°æ®åº“å®Œæ•´æ€§
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("PRAGMA integrity_check")
            result = cursor.fetchone()[0]
            conn.close()
            
            if result != "ok":
                print(f"âŒ æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {result}")
                return False
            
            print("âœ… å­˜å‚¨éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"éªŒè¯å­˜å‚¨å¤±è´¥: {str(e)}")
            return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¹ å­˜å‚¨æ¸…ç†å’Œç®¡ç†å·¥å…·")
    print("=" * 50)
    
    manager = StorageManager()
    
    # åˆ†æå½“å‰çŠ¶æ€
    analysis = manager.analyze_storage()
    manager.print_analysis(analysis)
    
    if analysis.get('total_records', 0) == 0:
        print("\nğŸ“­ å­˜å‚¨ä¸ºç©ºï¼Œæ— éœ€æ¸…ç†")
        return
    
    print("\nğŸ› ï¸  å¯ç”¨æ“ä½œ:")
    print("1. æ¸…ç†é‡å¤æ•°æ®")
    print("2. æŒ‰æ¥æºç®¡ç†æ•°æ®")
    print("3. é‡å»ºå‘é‡ç´¢å¼•")
    print("4. éªŒè¯å­˜å‚¨ä¸€è‡´æ€§")
    print("5. å®Œå…¨æ¸…ç©ºå­˜å‚¨")
    print("0. é€€å‡º")
    
    while True:
        choice = input("\nè¯·é€‰æ‹©æ“ä½œ (0-5): ").strip()
        
        if choice == "0":
            break
        elif choice == "1":
            removed = manager.clean_duplicates()
            if removed > 0:
                print(f"âœ… æ¸…ç†äº† {removed} ä¸ªé‡å¤è®°å½•")
        elif choice == "2":
            manager.clean_by_source()
        elif choice == "3":
            manager.rebuild_indexes()
        elif choice == "4":
            manager.validate_storage()
        elif choice == "5":
            confirm = input("ç¡®è®¤è¦æ¸…ç©ºæ‰€æœ‰å­˜å‚¨å—ï¼Ÿ(yes/no): ").strip().lower()
            if confirm == "yes":
                manager.vector_store.clear()
                print("âœ… å­˜å‚¨å·²æ¸…ç©º")
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")

if __name__ == "__main__":
    main()
