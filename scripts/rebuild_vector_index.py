#!/usr/bin/env python3
"""
é‡å»ºå‘é‡ç´¢å¼•çš„è„šæœ¬

è¯¥è„šæœ¬ä¼šï¼š
1. è¯»å–æ›´æ–°åçš„å›¾åƒæ•°æ®
2. é‡æ–°ç”Ÿæˆå‘é‡embeddings
3. é‡å»ºå‘é‡ç´¢å¼•
"""

import os
import sys
import sqlite3
import json
import logging
from pathlib import Path
from typing import Dict, Any, List

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from multimodal_rag.storage.vector_store import MultimodalVectorStore
from multimodal_rag.storage.metadata_store import MetadataStore
from multimodal_rag.processors.smart_chunker import DocumentChunk

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class VectorIndexRebuilder:
    """å‘é‡ç´¢å¼•é‡å»ºå™¨"""
    
    def __init__(self, storage_path: str = "./webui_storage"):
        self.storage_path = storage_path
        self.db_path = os.path.join(storage_path, "metadata", "metadata.db")
        
        # åˆå§‹åŒ–å­˜å‚¨ç»„ä»¶
        self.metadata_store = MetadataStore(os.path.join(storage_path, "metadata"))
        self.vector_store = MultimodalVectorStore(storage_path)
    
    def get_all_chunks(self) -> List[DocumentChunk]:
        """è·å–æ‰€æœ‰æ–‡æ¡£å—"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                SELECT content, chunk_type, metadata_json 
                FROM metadata 
                ORDER BY page_num, chunk_index
            ''')
            
            chunks = []
            for row in cursor.fetchall():
                content, chunk_type, metadata_json = row
                metadata = json.loads(metadata_json)
                
                # åˆ›å»ºDocumentChunkå¯¹è±¡
                chunk = DocumentChunk(
                    content=content,
                    chunk_type=chunk_type,
                    metadata=metadata,
                    token_count=metadata.get('token_count', 0)
                )
                chunks.append(chunk)
            
            conn.close()
            logger.info(f"è·å–åˆ° {len(chunks)} ä¸ªæ–‡æ¡£å—")
            return chunks
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£å—å¤±è´¥: {str(e)}")
            return []
    
    def clear_existing_indexes(self):
        """æ¸…ç©ºç°æœ‰ç´¢å¼•"""
        try:
            # åˆ é™¤ç´¢å¼•æ–‡ä»¶
            index_files = [
                os.path.join(self.storage_path, "text_index.faiss"),
                os.path.join(self.storage_path, "image_index.faiss")
            ]
            
            for index_file in index_files:
                if os.path.exists(index_file):
                    os.remove(index_file)
                    logger.info(f"åˆ é™¤ç´¢å¼•æ–‡ä»¶: {index_file}")
            
            # é‡æ–°åˆå§‹åŒ–å‘é‡å­˜å‚¨
            self.vector_store = MultimodalVectorStore(self.storage_path)
            logger.info("å‘é‡å­˜å‚¨é‡æ–°åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºç´¢å¼•å¤±è´¥: {str(e)}")
    
    def rebuild_indexes(self):
        """é‡å»ºå‘é‡ç´¢å¼•"""
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£å—
            chunks = self.get_all_chunks()
            if not chunks:
                logger.error("æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£å—ï¼Œæ— æ³•é‡å»ºç´¢å¼•")
                return
            
            # æ¸…ç©ºç°æœ‰ç´¢å¼•
            self.clear_existing_indexes()
            
            # æŒ‰æ¥æºåˆ†ç»„
            chunks_by_source = {}
            for chunk in chunks:
                source = chunk.metadata.get('source', 'unknown')
                if source not in chunks_by_source:
                    chunks_by_source[source] = []
                chunks_by_source[source].append(chunk)
            
            # é‡æ–°æ·»åŠ æ‰€æœ‰å—
            total_added = 0
            for source, source_chunks in chunks_by_source.items():
                logger.info(f"é‡å»ºç´¢å¼•: {source} ({len(source_chunks)} ä¸ªå—)")
                self.vector_store.add_chunks(source_chunks, source)
                total_added += len(source_chunks)
            
            logger.info(f"å‘é‡ç´¢å¼•é‡å»ºå®Œæˆ: æ€»å…±å¤„ç† {total_added} ä¸ªæ–‡æ¡£å—")
            
        except Exception as e:
            logger.error(f"é‡å»ºç´¢å¼•å¤±è´¥: {str(e)}")
    
    def test_search(self):
        """æµ‹è¯•æœç´¢åŠŸèƒ½"""
        try:
            # æµ‹è¯•å›¾åƒæœç´¢
            logger.info("æµ‹è¯•å›¾åƒæœç´¢...")
            results = self.vector_store.search(
                query="ç¬¬ä¸€å¼ å›¾ç‰‡",
                top_k=5,
                search_type="image"
            )
            
            logger.info(f"å›¾åƒæœç´¢ç»“æœ: {len(results)} ä¸ª")
            for i, result in enumerate(results[:3]):
                logger.info(f"ç»“æœ {i+1}: {result.get('content', '')[:100]}...")
            
            # æµ‹è¯•æ–‡æœ¬æœç´¢
            logger.info("æµ‹è¯•æ–‡æœ¬æœç´¢...")
            results = self.vector_store.search(
                query="å…¬å¼",
                top_k=5,
                search_type="both"
            )
            
            logger.info(f"ç»¼åˆæœç´¢ç»“æœ: {len(results)} ä¸ª")
            for i, result in enumerate(results[:3]):
                logger.info(f"ç»“æœ {i+1}: {result.get('content', '')[:100]}...")
            
        except Exception as e:
            logger.error(f"æµ‹è¯•æœç´¢å¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”„ å‘é‡ç´¢å¼•é‡å»ºå·¥å…·")
    print("=" * 50)
    
    rebuilder = VectorIndexRebuilder()
    
    print("ğŸš€ å¼€å§‹é‡å»ºå‘é‡ç´¢å¼•...")
    rebuilder.rebuild_indexes()
    
    print("ğŸ§ª æµ‹è¯•æœç´¢åŠŸèƒ½...")
    rebuilder.test_search()
    
    print("âœ… å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()
